{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pip install ipywidgets --upgrade\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install ipywidgets --upgrade\n",
    "# !pip install -r /dccstor/fluidos/luba_dev/fluidos-model-orchestrator/requirements-dev.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# import sys\n",
    "# base_path = Path(os.path.abspath(\"\")).parent.parent\n",
    "# sys.path.append(base_path.as_posix())\n",
    "# base_path.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install /dccstor/fluidos/luba_dev/fluidos-model-orchestrator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "import os\n",
    "\n",
    "import numpy as np  # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import tensorflow as tf  # type: ignore\n",
    "import torch  # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fluidos_model_orchestrator.data_pipeline.augmentation.augmentation_utils import AUGMENTATION_TYPES\n",
    "class AUGMENTATION_TYPES:\n",
    "    PERFORMANCE_RATING = \"performance_rating\"\n",
    "    FEEDBACK_LOOP = \"feedback_loop\"\n",
    "# from fluidos_model_orchestrator.data_pipeline.augmentation.augmentation_pipeline import create_augmented_dataset_df\n",
    "# from fluidos_model_orchestrator.data_pipeline.data_processor_factory import DataProcessorFactory\n",
    "from fluidos_model_orchestrator.data_pipeline.data_util import FLUIDOS_DATASETS\n",
    "from fluidos_model_orchestrator.data_pipeline.data_util import get_target_column\n",
    "from fluidos_model_orchestrator.data_pipeline.data_util import load_ml_ready_df\n",
    "from fluidos_model_orchestrator.model.utils import MODEL_TYPES\n",
    "from fluidos_model_orchestrator.model_pipeline.model_trainer_factory import ModelTrainerFactory\n",
    "# from fluidos_model_orchestrator.util import get_default_output_dir_path\n",
    "# from fluidos_model_orchestrator.util import reset_output_dir\n",
    "from fluidos_model_orchestrator.model.utils import FLUIDOS_COL_NAMES\n",
    "\n",
    "import ipywidgets as widgets\n",
    "widgets.IntSlider()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = Path(os.getcwd()).parent.parent\n",
    "\n",
    "platform = \"ccc\"\n",
    "dataset_name = FLUIDOS_DATASETS.BITBRAINS  # FLUIDOS_DATASETS.GCT\n",
    "model_type = MODEL_TYPES.CG  # MODEL_TYPES.FLUIDOS_RANKER\n",
    "augmentation = AUGMENTATION_TYPES.FEEDBACK_LOOP  # AUGMENTATION_TYPES.PERFORMANCE_RATING\n",
    "\n",
    "epochs = 5\n",
    "load_from_generated = 0\n",
    "test_mode = False\n",
    "max_pod = 2000\n",
    "tr_number = 50 # -1 for gct\n",
    "\n",
    "# path_output_model = \"/dccstor/fluidos/luba_dev/tid_bitbrains_100k/\"\n",
    "# path_output = \"/dccstor/fluidos/luba_dev/tid_bitbrains_100k\"\n",
    "path_output_model = \"/Users/killianlevacher/Downloads/tmp_fluidos/tid_bitbrains_100k/\"\n",
    "path_output = \"/Users/killianlevacher/Downloads/tmp_fluidos/tid_bitbrains_100k\"\n",
    "\n",
    "\n",
    "dataset_subset = \"rnd\"\n",
    "# path_dataset_ml_ready = None\n",
    "# path_dataset_ml_ready = \"/dccstor/fluidos/luba_dev/tid_bitbrains_100k/dataset/bitbrains_v0_train/ml_ready_augmented/feedback_loop\"\n",
    "path_dataset_ml_ready = '/Users/killianlevacher/Downloads/tmp_fluidos/feedback_loop'\n",
    "\n",
    "MODEL_SOURCE = \"model_source\"\n",
    "FED_MODELS_TRAINED = [MODEL_SOURCE, \"model_B\", \"model_C\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def get_default_dev_datasets_path(dataset_name: str) -> str:\n",
    "#     # fluidos-model-orchestrator\n",
    "#     return Path(f\"/dccstor/fluidos/datasets/{dataset_name}/original\").as_posix()\n",
    "#     # return Path(ROOT_DIR, f\"tests/dataset_resources/original_dataset_{dataset_name}\").as_posix()\n",
    "\n",
    "\n",
    "# def create_default_dev_augmented_ml_ready_dataset(dataset_name: str,\n",
    "#                                                   subset: str,\n",
    "#                                                   tmp_test_path: Path,\n",
    "#                                                   augmentation_types: list[str],\n",
    "#                                                   max_pods: int = 10000) -> tuple[Path, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "#     dataset_processor = DataProcessorFactory().create_dataset_processor(\n",
    "#         dataset_name,\n",
    "#         original_dataset_path=Path(get_default_dev_datasets_path(dataset_name)),\n",
    "#         output_dataset_path=tmp_test_path,\n",
    "#         dataset_version=\"0\",\n",
    "#         pods_number=max_pods,\n",
    "#         mode=\"train\",\n",
    "#         tr_number=tr_number\n",
    "#     )\n",
    "#     dataset_processor.create_source_dataset(subset=subset, cache=True)\n",
    "\n",
    "#     # #TODO probably can remove datasetname\n",
    "#     dataset_processor.create_ml_ready_dataset_df(cache=True)\n",
    "#     ml_augmented_path, _ = create_augmented_dataset_df(augmentation_types,\n",
    "#                                                        dataset_processor.ml_ready_path,\n",
    "#                                                        dataset_processor.ml_ready_augmented_path,\n",
    "#                                                        dataset_processor.metadata_obj,\n",
    "#                                                        dataset_name,\n",
    "#                                                        cache=True)\n",
    "\n",
    "#     ml_ready_augmented_path = ml_augmented_path.joinpath(augmentation_types[0])\n",
    "#     pods_assigment_df, template_resources_df = load_ml_ready_df(ml_ready_augmented_path)\n",
    "\n",
    "#     return ml_ready_augmented_path, pods_assigment_df, template_resources_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(df: pd.DataFrame, model_names: list[str], model_source_name: str, train_ratio: float = 0.2, test_ratio: float = 0.05) -> dict[str, dict[str, Any]]:\n",
    "\n",
    "    manifests = df[[FLUIDOS_COL_NAMES.POD_FILE_NAME, FLUIDOS_COL_NAMES.POD_MANIFEST]]\n",
    "    counts = manifests[FLUIDOS_COL_NAMES.POD_MANIFEST].value_counts().to_dict()\n",
    "    # Estimation of pod manifests distribution\n",
    "    frequency_per_pod = []\n",
    "    for index, row in manifests.iterrows():\n",
    "        frequency_per_pod.append(counts[row[FLUIDOS_COL_NAMES.POD_MANIFEST]])\n",
    "\n",
    "    manifests.insert(2, \"frequency\", frequency_per_pod)\n",
    "    manifests = manifests.sort_values(by=\"frequency\", ascending=False)\n",
    "    manifests[\"frequency\"] = manifests[\"frequency\"] / len(manifests)\n",
    "    manifests.index = range(len(manifests.index))\n",
    "    manifests = manifests.reset_index()\n",
    "\n",
    "    manifests_size = len(manifests)\n",
    "\n",
    "    train_size = int(train_ratio * manifests_size)\n",
    "    test_size = int(test_ratio * manifests_size) \n",
    "    test_manifests = manifests.sample(test_size)\n",
    "\n",
    "    train_manifests = manifests[:train_size]\n",
    "    local_manifests = manifests[train_size:]\n",
    "\n",
    "    train_pods = df[df[FLUIDOS_COL_NAMES.POD_FILE_NAME].isin(train_manifests[FLUIDOS_COL_NAMES.POD_FILE_NAME])]\n",
    "    local_pods = df[df[FLUIDOS_COL_NAMES.POD_FILE_NAME].isin(local_manifests[FLUIDOS_COL_NAMES.POD_FILE_NAME])]\n",
    "    test_pods = df[df[FLUIDOS_COL_NAMES.POD_FILE_NAME].isin(test_manifests[FLUIDOS_COL_NAMES.POD_FILE_NAME])]\n",
    "    split_size = len(local_pods) // (len(model_names) - 1)\n",
    "    local_pods_chunks = [local_pods[i:i + split_size] for i in range(0, len(local_pods), split_size)]\n",
    "\n",
    "    model_attributes = {}\n",
    "    model_attributes[MODEL_SOURCE] = {\"df_train\": train_pods, \"df_test\": test_pods}\n",
    "    for index, model_name in enumerate(model_names[1:]):\n",
    "        model_attributes[model_name] = {\"df_train\": local_pods_chunks[index],\n",
    "                                        \"df_test\": test_pods}\n",
    "            \n",
    "    return model_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.info(\"Starting Model Building and Training\")\n",
    "\n",
    "# if path_output is None:\n",
    "#     path_output = str(get_default_output_dir_path())\n",
    "\n",
    "# if platform != \"ccc\":\n",
    "#     reset_output_dir(Path(path_output))\n",
    "\n",
    "# '/Users/killianlevacher/GIT/project-fluidos/tests/dataset_resources/sample_dataset_GCT'\n",
    "# /Users/killianlevacher/GIT/project-fluidos/                           tests/dataset_resources/sample_dataset_GCT\n",
    "# /Users/killianlevacher/GIT/project-fluidos/fluidos-model-orchestrator/tests/dataset_resources/sample_dataset_GCT/table-a.csv\n",
    "# if path_dataset_ml_ready is None:\n",
    "#     ml_ready_dataset_path, pods_assigment_df, template_resources_df = create_default_dev_augmented_ml_ready_dataset(dataset_name,\n",
    "#                                                                                                                     dataset_subset,\n",
    "#                                                                                                                     Path(path_output).joinpath(\"dataset\"),\n",
    "#                                                                                                                     [augmentation],\n",
    "#                                                                                                                     max_pod)\n",
    "# else:\n",
    "ml_ready_dataset_path = Path(path_dataset_ml_ready)\n",
    "\n",
    "pods_assigment_df, template_resources_df = load_ml_ready_df(ml_ready_dataset_path)\n",
    "\n",
    "#TODO TMP - remove this when all tested \n",
    "pods_assigment_df = pods_assigment_df[:10000]\n",
    "\n",
    "model_attributes = split_dataset(pods_assigment_df, FED_MODELS_TRAINED, MODEL_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pods_assigment_df## Setting up model Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name == FLUIDOS_DATASETS.BITBRAINS:\n",
    "    total_pods = max_pod * tr_number\n",
    "else:\n",
    "    total_pods = max_pod\n",
    "    \n",
    "for model_name in FED_MODELS_TRAINED:\n",
    "    model_trainer = ModelTrainerFactory.create_model_trainer(model_type,\n",
    "                                                                ml_ready_dataset_path,\n",
    "                                                                Path(path_output).joinpath(\"model_training\"),\n",
    "                                                                max_pod=total_pods,\n",
    "                                                                epochs=epochs,\n",
    "                                                                target_column=get_target_column(augmentation),\n",
    "                                                                load_from_generated=load_from_generated,\n",
    "                                                                # load_from_generated=True,\n",
    "                                                                model_name=model_name)\n",
    "    model_trainer.prepare_directories()\n",
    "    model_attributes[model_name][\"trainer\"] = model_trainer\n",
    "\n",
    "for model_name in FED_MODELS_TRAINED:\n",
    "    TRAINING_RATIO = 1.0\n",
    "    cached_train = model_attributes[model_name][\"trainer\"].prepare_dataset(model_attributes[model_name]['df_train'],\n",
    "                                                                                template_resources_df, test_mode,\n",
    "                                                                                training_size_ratio=TRAINING_RATIO,\n",
    "                                                                                model_tag=model_name, dataset_type=\"train\")\n",
    "    cached_test = model_attributes[model_name][\"trainer\"].prepare_dataset(model_attributes[model_name]['df_test'],\n",
    "                                                                                template_resources_df, test_mode,\n",
    "                                                                                training_size_ratio=TRAINING_RATIO,\n",
    "                                                                                model_tag=model_name, dataset_type=\"test\")\n",
    "    model_attributes[model_name][\"trainer\"].cached_train = cached_train\n",
    "    model_attributes[model_name][\"trainer\"].cached_test = cached_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model_attributes[MODEL_SOURCE][\"trainer\"].build_model()\n",
    "model_attributes[MODEL_SOURCE][\"trainer\"].check_model_dataset_depencies()\n",
    "model_attributes[MODEL_SOURCE][\"trainer\"].train_model()\n",
    "model_attributes[MODEL_SOURCE][\"eval_results\"] = model_attributes[MODEL_SOURCE][\"trainer\"].evaluate()\n",
    "\n",
    "for model_name in FED_MODELS_TRAINED:\n",
    "    if model_name == MODEL_SOURCE:\n",
    "        continue\n",
    "    shutil.copytree(model_attributes[MODEL_SOURCE][\"trainer\"].checkpoint_path.parent.parent.parent, model_attributes[model_name][\"trainer\"].checkpoint_path.parent.parent.parent, dirs_exist_ok=True)\n",
    "    # shutil.copytree(model_attributes[MODEL_SOURCE][\"trainer\"].checkpoint_path.parent.parent, model_attributes[model_name][\"trainer\"].checkpoint_path.parent.parent, dirs_exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Child Models from Source Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for model_name in FED_MODELS_TRAINED:\n",
    "    if model_name == MODEL_SOURCE:\n",
    "        continue\n",
    "    print(model_name)\n",
    "    model_attributes[model_name][\"trainer\"].load_model(load_from_checkpoint=True)  # type: ignore\n",
    "    model_attributes[model_name][\"trainer\"].train_model()\n",
    "    model_attributes[model_name][\"eval_results\"] = model_attributes[model_name][\"trainer\"].evaluate()\n",
    "\n",
    "logging.info(\"MODELS Evaluation Results\")\n",
    "for model_name in FED_MODELS_TRAINED:\n",
    "    if \"eval_results\" in model_attributes[model_name]:\n",
    "        logging.info(f\"MODEL: {model_name}: {model_attributes[model_name]['eval_results']}\")\n",
    "\n",
    "logging.info(f\"Pipeline output located at: {path_output_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == MODEL_TYPES.CG:\n",
    "    results_df = pd.DataFrame(index=['accuracy', 'total_loss'], columns=FED_MODELS_TRAINED)\n",
    "\n",
    "    for model_name in FED_MODELS_TRAINED:\n",
    "        for metric, value in model_attributes[model_name][\"eval_results\"].items():\n",
    "            results_df.loc[metric, model_name] = value\n",
    "elif MODEL_TYPES.FLUIDOS_RANKER:\n",
    "    results_df = pd.DataFrame(index=['root_mean_squared_error','loss','regularization_loss','total_loss'], columns=FED_MODELS_TRAINED)\n",
    "\n",
    "    for model_name in FED_MODELS_TRAINED:\n",
    "        for metric, value in model_attributes[model_name][\"eval_results\"].items():\n",
    "            results_df.loc[metric, model_name] = value\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "\n",
    "bar_width = 0.2\n",
    "bar_positions = np.arange(len(results_df.columns))\n",
    "\n",
    "\n",
    "for i, index in enumerate(results_df.index):\n",
    "    plt.bar(\n",
    "        bar_positions + i * bar_width,\n",
    "        results_df.loc[index],\n",
    "        width=bar_width,\n",
    "        label=index\n",
    "    )\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Model Performance')\n",
    "plt.xticks(bar_positions + bar_width, results_df.columns)\n",
    "plt.legend(title='Indices', loc='lower right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
